{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "We represent each sample used for generating the decision tree as a Training Instance\n",
    "\n",
    "Each instance has the corresponding features: Pclass, Sex, Age, Siblings/Spouses Aboard, Parents/Children Aboard,\n",
    "     Fare, and Survived\n",
    "     \n",
    "'''\n",
    "class TrainingInstance:\n",
    "    \n",
    "\n",
    "    #Input: Pclass, Sex (gender), Age, Siblings/Spouses Aboard, Parents/Children Aboard, Fare, and Survived\n",
    "    def __init__(self, p_class, gender, age, siblings_Spouses, parents_Children, fare, survived):\n",
    "        self.p_class = p_class\n",
    "        self.gender = gender\n",
    "        self.age = age\n",
    "        self.siblings_Spouses = siblings_Spouses\n",
    "        self.parents_Children = parents_Children\n",
    "        self.fare = fare\n",
    "        self.survived = survived\n",
    "\n",
    "    \n",
    "    def __str__(self):\n",
    "        return \"p_class: {}, gender: {}, age: {}, sibling_spouses: {}, parents_children: {}, fare: {}, survived: {}\".format(self.p_class, self.gender, self.age, self.siblings_Spouses, self.parents_Children, self.fare, self.survived)\n",
    "    \n",
    "'''\n",
    "For a feature vector that we want a prediction for using the decision tree, we will represent it as a Test Instance\n",
    "The difference from training instance is that we don't have the survived parameter in this case\n",
    "'''\n",
    "class TestInstance:\n",
    "    \n",
    "    #Input: Pclass, Sex (gender), Age, Siblings/Spouses Aboard, Parents/Children Aboard, Fare\n",
    "     def __init__(self, p_class, gender, age, siblings_Spouses, parents_Children, fare):\n",
    "        self.p_class = p_class\n",
    "        self.gender = gender\n",
    "        self.age = age\n",
    "        self.siblings_Spouses = siblings_Spouses\n",
    "        self.parents_Children = parents_Children\n",
    "        self.fare = fare\n",
    "    \n",
    "     def __str__(self):\n",
    "        return \"p_class: {}, gender: {}, age: {}, sibling_spouses: {}, parents_children: {}, fare: {}\".format(self.p_class, self.gender, self.age, self.siblings_Spouses, self.parents_Children, self.fare)\n",
    "    \n",
    "    \n",
    "'''\n",
    "We will use the TreeNode data structure to represent the decision tree\n",
    "\n",
    "We have two types of Nodes:\n",
    "\n",
    "1) Non-Leaf node: will be used to represent the feature to split on\n",
    "2) Leaf node: will be used to represent the present for this path in the decision tree\n",
    "\n",
    "'''    \n",
    "class TreeNode:\n",
    "    \n",
    "    '''\n",
    "    Input: is_Leaf - boolean variable to determine whether a node is a leaf or not\n",
    "    '''\n",
    "    def __init__(self, is_Leaf):\n",
    "        \n",
    "        '''\n",
    "        val is not None if TreeNode is a non-leaf node. this variable stores the value associated with the feature to split\n",
    "        1 - Pclass, 2 - Sex, 3 - Age, 4 - Siblings/Spouses Aboard, 5 - Parents/Children Aboard, 6 - Fare\n",
    "        '''\n",
    "        self.val = None\n",
    "        \n",
    "        '''\n",
    "        left child stores the Treenode that corresponds to when the instance's value for the feature (val) is 0\n",
    "        '''\n",
    "        self.left = None\n",
    "        \n",
    "        '''\n",
    "        right child stores the Treenode that corresponds to when instance's value for the feature (val) is 1\n",
    "        '''\n",
    "        self.right = None\n",
    "        \n",
    "        '''\n",
    "        boolean variable that is used to check if node is a non-leaf or leaf node\n",
    "        '''\n",
    "        self.is_Leaf = is_Leaf\n",
    "        \n",
    "        '''\n",
    "        prediction is not None for a leaf node. Stores the prediction for the corresponding path in the decision tree\n",
    "        '''\n",
    "        self.prediction = None\n",
    "    \n",
    "    \n",
    "    #Method to set val (Feature to split on) for a non-leaf node\n",
    "    def add_val(self, val):\n",
    "        self.val = val\n",
    "    \n",
    "    #Method to set left child for a non-leaf node\n",
    "    def add_left(self, left_child):\n",
    "        self.left = left_child\n",
    "    \n",
    "    #Method to set right child for a non-leaf node\n",
    "    def add_right(self, right_child):\n",
    "        self.right = right_child\n",
    "    \n",
    "    #Method to set prediction for a leaf node\n",
    "    def add_prediction(self, prediction):\n",
    "        self.prediction = prediction\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "compute_entropy is a function to compute the entropy for a specific feature\n",
    "\n",
    "Input: 1) instances - a list of Training_Instances\n",
    "       2) feature - the specific feature for which we are calculating entropy\n",
    "       \n",
    "Output: the entropy for a specific feature\n",
    "'''\n",
    "def compute_entropy(instances, feature):\n",
    "    \n",
    "    num_zero = 0 \n",
    "    num_one = 0\n",
    "    num_total = len(instances)\n",
    "    \n",
    "    \n",
    "    if(feature == 1):\n",
    "        \n",
    "        for elem in instances:\n",
    "            if(elem.p_class == 0):\n",
    "                num_zero += 1\n",
    "            else:\n",
    "                num_one += 1\n",
    "    \n",
    "    elif(feature == 2):\n",
    "        \n",
    "        for elem in instances:\n",
    "            if(elem.gender == 0):\n",
    "                num_zero += 1\n",
    "            else:\n",
    "                num_one += 1\n",
    "    \n",
    "    \n",
    "    elif(feature == 3):\n",
    "        \n",
    "        for elem in instances:\n",
    "            if(elem.age == 0):\n",
    "                num_zero += 1\n",
    "            else:\n",
    "                num_one += 1\n",
    "    \n",
    "    elif(feature == 4):\n",
    "        \n",
    "        for elem in instances:\n",
    "            if(elem.siblings_Spouses == 0):\n",
    "                num_zero += 1\n",
    "            else:\n",
    "                num_one += 1\n",
    "    \n",
    "    elif(feature == 5):\n",
    "        \n",
    "        for elem in instances:\n",
    "            if(elem.parents_Children == 0):\n",
    "                num_zero += 1\n",
    "            else:\n",
    "                num_one += 1\n",
    "    \n",
    "    elif(feature == 6):\n",
    "        \n",
    "        for elem in instances:\n",
    "            if(elem.fare == 0):\n",
    "                num_zero += 1\n",
    "            else:\n",
    "                num_one += 1\n",
    "                \n",
    "    \n",
    "\n",
    "\n",
    "    probability_zero = float(num_zero) / float(num_total) \n",
    "    probability_one =  float(num_one) / float(num_total)\n",
    "    \n",
    "    if(probability_zero == 0 or probability_one == 0):\n",
    "        return 0\n",
    "    \n",
    "    \n",
    "    entropy = (-probability_zero * math.log2(probability_zero)) + (-probability_one * math.log2(probability_one))\n",
    "    return entropy\n",
    "   \n",
    "\n",
    "    \n",
    "'''\n",
    "split is a method to perform a split on a set of samples based on a specific feature\n",
    "\n",
    "Input: 1) instances - a list of Training_Instances\n",
    "     \n",
    "       2) feature - a specific feature to split on\n",
    "\n",
    "Ouput: 1) feature_is_zero - a list that contains the set of samples for which the sample's value for \n",
    "                            the feature on which we're spliiting on is 0\n",
    "       \n",
    "       2) feature_is_one - a list that contains the set of sample for which the sample's value for the \n",
    "                           feature we're splitting on is 1\n",
    "                           \n",
    "'''  \n",
    "def split(instances, feature):\n",
    "    \n",
    "    feature_is_zero = []\n",
    "    feature_is_one = []\n",
    "    \n",
    "    \n",
    "    if(feature == 1):\n",
    "        \n",
    "        for elem in instances:\n",
    "            \n",
    "            if(elem.p_class == 0):\n",
    "                feature_is_zero.append(elem)\n",
    "            elif(elem.p_class == 1):\n",
    "                feature_is_one.append(elem)\n",
    "        \n",
    "    elif(feature == 2):\n",
    "        \n",
    "        for elem in instances:\n",
    "            \n",
    "            if(elem.gender == 0):\n",
    "                feature_is_zero.append(elem)\n",
    "            elif(elem.gender == 1):\n",
    "                feature_is_one.append(elem)\n",
    "                \n",
    "    elif(feature == 3):\n",
    "        \n",
    "        for elem in instances:\n",
    "            \n",
    "            if(elem.age == 0):\n",
    "                feature_is_zero.append(elem)\n",
    "            elif(elem.age == 1):\n",
    "                feature_is_one.append(elem)\n",
    "    \n",
    "    elif(feature == 4):\n",
    "        \n",
    "        for elem in instances:\n",
    "            \n",
    "            if(elem.siblings_Spouses == 0):\n",
    "                feature_is_zero.append(elem)\n",
    "            elif(elem.siblings_Spouses == 1):\n",
    "                feature_is_one.append(elem)\n",
    "    \n",
    "    elif(feature == 5):\n",
    "        \n",
    "        for elem in instances:\n",
    "            \n",
    "            if(elem.parents_Children == 0):\n",
    "                feature_is_zero.append(elem)\n",
    "            elif(elem.parents_Children == 1):\n",
    "                feature_is_one.append(elem)\n",
    "    \n",
    "    elif(feature == 6):\n",
    "        \n",
    "        for elem in instances:\n",
    "            \n",
    "            if(elem.fare == 0):\n",
    "                feature_is_zero.append(elem)\n",
    "            elif(elem.fare == 1):\n",
    "                feature_is_one.append(elem)\n",
    "            \n",
    "    \n",
    "    return feature_is_zero, feature_is_one\n",
    "\n",
    "\n",
    "    \n",
    "'''\n",
    "compute_entropy_after_split is a method to compute the the conditional entropy on y after split, H(X|Y)\n",
    "\n",
    "Input: 1) feature_is_zero - a list that contains the set of samples for which the sample's value for \n",
    "                            the feature on which we're spliiting on is 0\n",
    "        \n",
    "       2) feature_is_one - a list that contains the set of sample for which the sample's value for the \n",
    "                           feature we're splitting on is 1\n",
    "       \n",
    "       3) num_instances - the number of samples at node before split\n",
    "       \n",
    "       4) num_survived - the number of samples at node for which survived = 1\n",
    "       \n",
    "       5) num_deceased - the number of samples at node for which survived = 0\n",
    "\n",
    "\n",
    "Output: The conditional entropy on y when splitting on the specific feature\n",
    "'''\n",
    "def compute_entropy_after_split(feature_is_zero, feature_is_one, num_instances, num_survived, num_deceased):\n",
    "    \n",
    "    num_x0_y0 = 0\n",
    "    num_x0_y1 = 0\n",
    "    num_x1_y0 = 0\n",
    "    num_x1_y1 = 0\n",
    "    \n",
    "    for elem in feature_is_zero:\n",
    "        \n",
    "        if elem.survived == 0:\n",
    "            num_x0_y0 += 1\n",
    "        elif elem.survived == 1:\n",
    "            num_x0_y1 += 1\n",
    "    \n",
    "    for elem in feature_is_one:\n",
    "        \n",
    "        if elem.survived == 0:\n",
    "            num_x1_y0 += 1\n",
    "        elif elem.survived == 1:\n",
    "            num_x1_y1 += 1\n",
    "    \n",
    "    \n",
    "    joint_x0_y0 = float(num_x0_y0) / float(num_instances)\n",
    "    joint_x0_y1 = float(num_x0_y1) / float(num_instances)\n",
    "    joint_x1_y0 = float(num_x1_y0) / float(num_instances)\n",
    "    joint_x1_y1 = float(num_x1_y1) / float(num_instances)\n",
    "    \n",
    "    cond_x0_y0 = float(num_x0_y0) / float(num_deceased)\n",
    "    cond_x0_y1 = float(num_x0_y1) / float(num_survived)\n",
    "    cond_x1_y0 = float(num_x1_y0) / float(num_deceased)\n",
    "    cond_x1_y1 = float(num_x1_y1) / float(num_survived)\n",
    "    \n",
    "    entropy = 0\n",
    "    \n",
    "    if(joint_x0_y0 != 0):\n",
    "        entropy += joint_x0_y0 * math.log2(1/cond_x0_y0)\n",
    "    \n",
    "    if(joint_x0_y1 != 0):\n",
    "        entropy += joint_x0_y1 * math.log2(1/cond_x0_y1)\n",
    "    \n",
    "    if(joint_x1_y0 != 0):\n",
    "        entropy += joint_x1_y0 * math.log2(1/cond_x1_y0)\n",
    "    \n",
    "    if(joint_x1_y1 != 0):\n",
    "        entropy += joint_x1_y1 * math.log2(1/cond_x1_y1)\n",
    "        \n",
    "   \n",
    "    return entropy\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "compute_mutual_information is a function that computes mutual information for a specific feature\n",
    "\n",
    "Input: 1) instances - a list of Training_Instances\n",
    "\n",
    "       2) feature - the feature for which we are trying to find mutual information\n",
    "       \n",
    "Output: Mutual information for a specific feature\n",
    "'''\n",
    "def compute_mutual_information(instances, feature):\n",
    "    \n",
    "    entropy = compute_entropy(instances, feature)\n",
    "    \n",
    "    feature_is_zero, feature_is_one = split(instances, feature)\n",
    "    \n",
    "    num_survived = 0\n",
    "    num_deceased = 0\n",
    "    \n",
    "    for elem in instances:\n",
    "        \n",
    "        if(elem.survived == 1):\n",
    "            num_survived +=1\n",
    "        elif(elem.survived == 0):\n",
    "            num_deceased += 1\n",
    "\n",
    "    \n",
    "    entropy_after_split = compute_entropy_after_split(feature_is_zero, feature_is_one, len(instances), num_survived, num_deceased)\n",
    "   \n",
    "    mutual_information = entropy - entropy_after_split\n",
    "\n",
    "    return mutual_information, feature_is_zero, feature_is_one\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "decision_tree_alg is the function that generates the decision tree\n",
    "\n",
    "Input: 1) instances - the list of training instances\n",
    "\n",
    "       2) splits_available - a list containing the features on which we can split on at a particular recursion level\n",
    "       \n",
    "       3) total_instances - the total number of samples at the beginning. Use this to perform early stopping when number of\n",
    "                          samples at current level < 5% of number of samples at the beginning \n",
    "       \n",
    "Output: The decision tree generated with respect to list of training instances and the list of features that we can split on \n",
    "\n",
    "'''\n",
    "def decision_tree_alg(instances, splits_available, total_instances):\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    Stopping Criteria - 1\n",
    "    \n",
    "    If no samples at node then predict 'deceased' by default\n",
    "    '''\n",
    "    if(len(instances) == 0):\n",
    "     \n",
    "        #We create a leaf node since we have run out of samples\n",
    "        ret_node = TreeNode(True)\n",
    "        ret_node.add_prediction(0)\n",
    "        \n",
    "        return ret_node\n",
    "    \n",
    "    \n",
    "    \n",
    "    '''\n",
    "    Stopping Criteria - 2\n",
    "    \n",
    "    Check if all samples have the same label\n",
    "    '''\n",
    "    is_Zero = True\n",
    "    is_One = True\n",
    "    \n",
    "    for elem in instances:\n",
    "        if(elem.survived == 1):\n",
    "            is_Zero = False\n",
    "        \n",
    "        if(elem.survived == 0):\n",
    "            is_One = False\n",
    "    \n",
    "    #Check if all samples have the label 0 (deceased)\n",
    "    if(is_Zero == True):\n",
    "        \n",
    "        #We create a leaf node with the prediction of deceased since all samples have the same label (deceased) of 0\n",
    "        ret_node = TreeNode(True)\n",
    "        ret_node.add_prediction(0)\n",
    "        \n",
    "        return ret_node\n",
    "\n",
    "    #Check if all samples have the label 1 (survived)\n",
    "    if(is_One == True):\n",
    "        \n",
    "        #We create a leaf node with the prediction of survived since all samples have the same label (survived) of 1\n",
    "        ret_node = TreeNode(True)\n",
    "        ret_node.add_prediction(1)\n",
    "     \n",
    "        return ret_node\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    Stopping Criteria - 3\n",
    "    \n",
    "    Check if there are no splits available \n",
    "    \n",
    "    We'll use majority vote (labels) and we'll predict the majority label\n",
    "    '''\n",
    "    if(len(splits_available) == 0):\n",
    "        \n",
    "        num_zero = 0\n",
    "        num_one = 0\n",
    "        \n",
    "        for elem in instances:\n",
    "            \n",
    "            if(elem.survived == 0):\n",
    "                num_zero += 1\n",
    "            else:\n",
    "                num_one += 1\n",
    "        \n",
    "        #Check if zero is the majority label\n",
    "        if(num_zero >= num_one):\n",
    "            \n",
    "            #We create a leaf node with the prediction of deceased\n",
    "            ret_node = TreeNode(True)\n",
    "            ret_node.add_prediction(0)\n",
    "    \n",
    "            return ret_node\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            #We create a leaf node with the prediction of survived\n",
    "            ret_node = TreeNode(True)\n",
    "            ret_node.add_prediction(1)\n",
    "        \n",
    "            return ret_node\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    Stopping Criteria - 4\n",
    "    \n",
    "    Check if the number of samples at current level is less than 5% of total number of samples\n",
    "     \n",
    "     We'll use majority vote (labels) and we'll predict the majority label\n",
    "    '''\n",
    "    if(len(instances) < (0.05 * total_instances)):\n",
    "        \n",
    "        num_zero = 0\n",
    "        num_one = 0\n",
    "        \n",
    "        for elem in instances:\n",
    "            \n",
    "            if(elem.survived == 0):\n",
    "                num_zero += 1\n",
    "            else:\n",
    "                num_one += 1\n",
    "        \n",
    "        #Check if zero is the majority label\n",
    "        if(num_zero >= num_one):\n",
    "            \n",
    "            #We create a leaf node with the prediction of deceased\n",
    "            ret_node = TreeNode(True)\n",
    "            ret_node.add_prediction(0)\n",
    "    \n",
    "            return ret_node\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            #We create a leaf node with the prediction of survived\n",
    "            ret_node = TreeNode(True)\n",
    "            ret_node.add_prediction(1)\n",
    "        \n",
    "            return ret_node\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    '''\n",
    "    Since the above stopping criteria have not been met, we'll try to find the best split by calculating\n",
    "          mutual information for all the features that we can split on\n",
    "    ''' \n",
    "    \n",
    "    info_gain = 0\n",
    "    best_feature = 0\n",
    "    f_z = None\n",
    "    f_o = None\n",
    "    \n",
    "    \n",
    "    #We iterate over the features that we can split on and compute the mutual information\n",
    "    for feature in splits_available:\n",
    "        \n",
    "        mutual_info, feature_zero, feature_one = compute_mutual_information(instances, feature)\n",
    "        \n",
    "        #We determine the best feature to split on\n",
    "        if(mutual_info > info_gain):\n",
    "            best_feature = feature\n",
    "            info_gain = mutual_info\n",
    "            f_z = feature_zero\n",
    "            f_o = feature_one\n",
    "      \n",
    "    \n",
    "    '''\n",
    "    In the case where the mutual information for all features is 0, we again use the majority label to predict\n",
    "    '''\n",
    "    if(best_feature == 0):\n",
    "        \n",
    "        num_zero = 0\n",
    "        num_one = 0\n",
    "        \n",
    "        for elem in instances:\n",
    "            \n",
    "            if(elem.survived == 0):\n",
    "                num_zero += 1\n",
    "            else:\n",
    "                num_one += 1\n",
    "        \n",
    "        #We create a leaf node with the prediction of deceased\n",
    "        if(num_zero >= num_one):\n",
    "            ret_node = TreeNode(True)\n",
    "            ret_node.add_prediction(0)\n",
    "        \n",
    "            return ret_node\n",
    "        \n",
    "        #We create a leaf node with the prediction of survived\n",
    "        else:\n",
    "            ret_node = TreeNode(True)\n",
    "            ret_node.add_prediction(1)\n",
    "          \n",
    "            return ret_node\n",
    "    \n",
    "    \n",
    "    \n",
    "    '''\n",
    "    We now handle the case where there is a best feature to split on\n",
    "    '''\n",
    "    \n",
    "    #We create a non-leaf node with the best feature to split on\n",
    "    curr = TreeNode(False)\n",
    "    curr.add_val(best_feature)\n",
    "    \n",
    "    \n",
    "    #We create the list of features we can split on for the next recursion level\n",
    "    new_feature_Set = []\n",
    "    \n",
    "    for item in splits_available:\n",
    "        \n",
    "        if(item != best_feature):\n",
    "            new_feature_Set.append(item)\n",
    "    \n",
    "    #The left child is a node for which the instances have value of 0 for the feature we split on\n",
    "    left_split = decision_tree_alg(f_z, new_feature_Set, total_instances)\n",
    "    curr.add_left(left_split)\n",
    "    \n",
    "    #The right child is a node for which the instances have value of 1 for the feature we split on\n",
    "    right_split = decision_tree_alg(f_o, new_feature_Set, total_instances)\n",
    "    curr.add_right(right_split)\n",
    "    \n",
    "    return curr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "level_order_Traversal is a function that perform a level order traversal on the decision tree\n",
    "\n",
    "Input: 1) root: the root node of the decision tree\n",
    "       2) level: the current level in the traversal\n",
    "       3) lists: a 2D list that stores the values for each level\n",
    "       \n",
    "Output: Updates the 'lists' to store all the values at each level\n",
    "'''\n",
    "def level_order_Traversal(root, level, lists):\n",
    "    \n",
    "    if(root == None):\n",
    "        return\n",
    "    \n",
    "    if(len(lists) == level):\n",
    "    \n",
    "        lists.append([])\n",
    "    \n",
    "    if(root.is_Leaf == True):\n",
    "        \n",
    "        if(root.prediction == 1):\n",
    "            lists[level].append(\"Survived\")\n",
    "        else:\n",
    "            lists[level].append(\"Deceased\")\n",
    "       \n",
    "        return\n",
    "    \n",
    "    lists[level].append(root.val)\n",
    "    level_order_Traversal(root.left, level+1, lists)\n",
    "    level_order_Traversal(root.right, level+1, lists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "cross_validation is a method that performs 10-fold cross validation \n",
    "\n",
    "Input: 1) instances - the list of instances on which we will perform CV\n",
    "\n",
    "Output: the average accuracy of 10-fold cross validation\n",
    "'''\n",
    "def cross_validation(instances):\n",
    "    \n",
    "    #Divide into 10 subsets \n",
    "    subset_1 = []\n",
    "    subset_2 = []\n",
    "    subset_3 = []\n",
    "    subset_4 = []\n",
    "    subset_5 = []\n",
    "    subset_6 = []\n",
    "    subset_7 = []\n",
    "    subset_8 = []\n",
    "    subset_9 = []\n",
    "    subset_10 = []\n",
    "    \n",
    "    #Since there are 887 samples we'll have 9 subsets that have 89 samples and 1 that has 86\n",
    "    temp = []\n",
    "    splits_available = [1,2,3,4,5,6]\n",
    "    \n",
    "    #Call balance_classes method (defined in next cell)\n",
    "    balanced_dist = balance_classes(instances)\n",
    "    \n",
    "   \n",
    "    \n",
    "    for i in range(len(balanced_dist)):\n",
    "        \n",
    "        if(i < 89):\n",
    "            subset_1.append(balanced_dist[i])\n",
    "        elif(i < 178):\n",
    "            subset_2.append(balanced_dist[i])\n",
    "        elif(i < 267):\n",
    "            subset_3.append(balanced_dist[i])\n",
    "        elif(i < 356):\n",
    "            subset_4.append(balanced_dist[i])\n",
    "        elif(i < 445):\n",
    "            subset_5.append(balanced_dist[i])\n",
    "        elif(i < 534):\n",
    "            subset_6.append(balanced_dist[i])\n",
    "        elif(i < 623):\n",
    "            subset_7.append(balanced_dist[i])\n",
    "        elif(i < 712):\n",
    "            subset_8.append(balanced_dist[i])\n",
    "        elif(i < 801):\n",
    "            subset_9.append(balanced_dist[i])\n",
    "        else:\n",
    "            subset_10.append(balanced_dist[i])\n",
    "    \n",
    "    \n",
    "    for element in balanced_dist:\n",
    "        temp.append(element)\n",
    "    \n",
    "    #drop subset 1\n",
    "    for i in range(89):\n",
    "        del temp[0]\n",
    "    \n",
    "    #generate decision tree using subsets 2-10\n",
    "    tree1 = decision_tree_alg(temp, splits_available, len(temp))\n",
    "    \n",
    "    #compute accuracy using subset 1 (compute_accuracy defined in next cell)\n",
    "    accuracy1 = compute_accuracy(tree1, subset_1)\n",
    "    print(accuracy1)\n",
    "    \n",
    "    \n",
    "    temp = []\n",
    "    for element in balanced_dist:\n",
    "        temp.append(element)\n",
    "    \n",
    "    #drop subset 2\n",
    "    for i in range(89):\n",
    "        del temp[89]\n",
    "        \n",
    "    #generate decision tree using subset 1 and subsets 3-10\n",
    "    tree2 = decision_tree_alg(temp, splits_available, len(temp))\n",
    "    \n",
    "    #compute accuracy using subset 2\n",
    "    accuracy2 = compute_accuracy(tree2, subset_2)\n",
    "    print(accuracy2)\n",
    "    \n",
    "    \n",
    "    temp = []\n",
    "    for element in balanced_dist:\n",
    "        temp.append(element)\n",
    "    \n",
    "    #drop subset 3\n",
    "    for i in range(89):\n",
    "        del temp[178]\n",
    "        \n",
    "    #generate decision tree using subset 1-2 and 4-10\n",
    "    tree3 = decision_tree_alg(temp, splits_available, len(temp))\n",
    "    \n",
    "    #compute accuracy using subset 3\n",
    "    accuracy3 = compute_accuracy(tree3, subset_3)\n",
    "    print(accuracy3)\n",
    "    \n",
    "    \n",
    "    temp = []\n",
    "    for element in balanced_dist:\n",
    "        temp.append(element)\n",
    "   \n",
    "    #drop subset 4\n",
    "    for i in range(89):\n",
    "        del temp[267]\n",
    "    \n",
    "    #generate decision tree using subset 1-3 and 5-10\n",
    "    tree4 = decision_tree_alg(temp, splits_available, len(temp))\n",
    "    \n",
    "    #compute accuracy using subset 4\n",
    "    accuracy4 = compute_accuracy(tree4, subset_4)\n",
    "    print(accuracy4)\n",
    "    \n",
    "    \n",
    "    temp = []\n",
    "    for element in balanced_dist:\n",
    "        temp.append(element)\n",
    "    \n",
    "    #drop subset 5\n",
    "    for i in range(89):\n",
    "        del temp[356]\n",
    "        \n",
    "    #generate decision tree using subset 1-4 and 6-10\n",
    "    tree5 = decision_tree_alg(temp, splits_available, len(temp))\n",
    "    \n",
    "    #compute accuracy using subset 5\n",
    "    accuracy5 = compute_accuracy(tree5, subset_5)\n",
    "    print(accuracy5)\n",
    "    \n",
    "    \n",
    "    temp = []\n",
    "    for element in balanced_dist:\n",
    "        temp.append(element)\n",
    "        \n",
    "    #drop subset 6\n",
    "    for i in range(89):\n",
    "        del temp[445]\n",
    "        \n",
    "    #generate decision tree using subset 1-5 and 7-10\n",
    "    tree6 = decision_tree_alg(temp, splits_available, len(temp))\n",
    "    \n",
    "    #compute accuracy using subset 6\n",
    "    accuracy6 = compute_accuracy(tree6, subset_6)\n",
    "    print(accuracy6)\n",
    "    \n",
    "    \n",
    "    temp = []\n",
    "    for element in balanced_dist:\n",
    "        temp.append(element)\n",
    "    \n",
    "    #drop subset 7\n",
    "    for i in range(89):\n",
    "        del temp[534]\n",
    "    \n",
    "    #generate decision tree using subset 1-6 and 8-10\n",
    "    tree7 = decision_tree_alg(temp, splits_available, len(temp))\n",
    "    \n",
    "    #compute accuracy using subset 7\n",
    "    accuracy7 = compute_accuracy(tree7, subset_7)\n",
    "    print(accuracy7)\n",
    "    \n",
    "    \n",
    "    temp = []\n",
    "    for element in balanced_dist:\n",
    "        temp.append(element)\n",
    "    \n",
    "    #drop subset 8\n",
    "    for i in range(89):\n",
    "        del temp[623]\n",
    "    \n",
    "    #generate decision tree using subset 1-7 and 9-10\n",
    "    tree8 = decision_tree_alg(temp, splits_available, len(temp))\n",
    "    \n",
    "    #compute accuracy using subset 8\n",
    "    accuracy8 = compute_accuracy(tree8, subset_8)\n",
    "    print(accuracy8)\n",
    "    \n",
    "    \n",
    "    temp = []\n",
    "    for element in balanced_dist:\n",
    "        temp.append(element)\n",
    "      \n",
    "    #drop subset 9\n",
    "    for i in range(89):\n",
    "        del temp[712]\n",
    "        \n",
    "    #generate decision tree using subset 1-8 and 10\n",
    "    tree9 = decision_tree_alg(temp, splits_available, len(temp))\n",
    "    \n",
    "    #compute accuracy using subset 9\n",
    "    accuracy9 = compute_accuracy(tree9, subset_9)\n",
    "    print(accuracy9)\n",
    "    \n",
    "    \n",
    "    temp = []\n",
    "    for element in balanced_dist:\n",
    "        temp.append(element)\n",
    "    \n",
    "    #drop subset 10\n",
    "    for i in range(86):\n",
    "        del temp[801]\n",
    "    \n",
    "    #generate decision tree using subset 1-9\n",
    "    tree10 = decision_tree_alg(temp, splits_available, len(temp))\n",
    "    \n",
    "    #compute accuracy using subset 10\n",
    "    accuracy10 = compute_accuracy(tree10, subset_10)\n",
    "    print(accuracy10)\n",
    "    \n",
    "    #compute average accuracy\n",
    "    total_accuracy = (accuracy1 + accuracy2 + accuracy3 + accuracy4 + accuracy5 + accuracy6 + accuracy7 + accuracy8 + accuracy9 + accuracy10) / 10\n",
    "    \n",
    "    print('total accuracy: ' + str(total_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "balance_classes is a method that balance classes for each subset before applying cross-validation\n",
    "\n",
    "Input: 1) the list of training instances\n",
    "\n",
    "Output: a list that balances the instances\n",
    "\n",
    "\n",
    "We know that 342 survived and 545 died\n",
    "In CV we'll use 9 subsets with 89 samples and one with 86\n",
    "\n",
    "For the 9 subsets with 89 samples we'll have:\n",
    "\n",
    "34 instances of survived\n",
    "55 instances of death\n",
    "\n",
    "For the 1 subset with 86 samples we'll have \n",
    "\n",
    "36 instances of survived\n",
    "50 instances of death\n",
    "\n",
    "'''\n",
    "def balance_classes(instances):\n",
    "    \n",
    "    survived = []\n",
    "    deceased = []\n",
    "    \n",
    "    for elem in instances:\n",
    "        \n",
    "        if (elem.survived == 1):\n",
    "            survived.append(elem)\n",
    "        else:\n",
    "            deceased.append(elem)\n",
    "    \n",
    "    \n",
    "    temp = []\n",
    "    survived_index = 0\n",
    "    death_index = 0\n",
    "    \n",
    "    #First 9 susbsets\n",
    "    for i in range(9):\n",
    "        \n",
    "        for j in range(34):\n",
    "            temp.append(survived[survived_index])\n",
    "            survived_index += 1\n",
    "            \n",
    "        \n",
    "        for k in range(55):\n",
    "            \n",
    "            temp.append(deceased[death_index])\n",
    "            death_index += 1\n",
    "    \n",
    "    \n",
    "    \n",
    "    #Last subset\n",
    "    for j in range(36):\n",
    "        temp.append(survived[survived_index])\n",
    "        survived_index+=1\n",
    "    \n",
    "    for k in range(50):\n",
    "        temp.append(deceased[death_index])\n",
    "        death_index += 1\n",
    "    \n",
    "    \n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "compute_accuracy is a method that can be used to determine the decision tree's prediction\n",
    "        accuracy for a subset of test samples\n",
    "        \n",
    "Input: 1) tree: the decision tree we'll be using to predict\n",
    "\n",
    "       2) subset: the test set we'll use to find the decision tree's accuracy\n",
    "    \n",
    "Output: the decision tree's accuracy over a test set\n",
    "'''\n",
    "def compute_accuracy(tree, subset):\n",
    "    \n",
    "    num_correct = 0\n",
    "    \n",
    "    #Iterate over the subset and compute accuracy\n",
    "    for elem in subset:\n",
    "        \n",
    "        #find the decision tree's prediction for the test instance (prediction method defined in next cell)\n",
    "        pred = prediction(tree, elem)\n",
    "        \n",
    "        if(pred == elem.survived):\n",
    "            num_correct += 1\n",
    "    \n",
    "    accuracy = float(num_correct) / len(subset)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "prediction is a method to determine the decision tree's prediction for a test instance\n",
    "\n",
    "Input: 1) tree - the decision tree we'll use to determine the prediction\n",
    "\n",
    "       2) instance - the test instance for which we want a prediction\n",
    "\n",
    "\n",
    "Output: the decision tree's prediction\n",
    "'''\n",
    "def prediction(tree, instance):\n",
    "    \n",
    "    if(tree.is_Leaf == True):\n",
    "        return tree.prediction\n",
    "    \n",
    "    if(tree.val == 1):\n",
    "        if(instance.p_class == 0):\n",
    "            return prediction(tree.left, instance)\n",
    "        else:\n",
    "            return prediction(tree.right, instance)\n",
    "    \n",
    "    elif(tree.val == 2):\n",
    "        if(instance.gender == 0):\n",
    "            return prediction(tree.left, instance)\n",
    "        else:\n",
    "            return prediction(tree.right, instance)\n",
    "    \n",
    "    elif(tree.val == 3):\n",
    "        if(instance.age == 0):\n",
    "            return prediction(tree.left, instance)\n",
    "        else:\n",
    "            return prediction(tree.right, instance)\n",
    "            \n",
    "    elif(tree.val == 4):\n",
    "        if(instance.siblings_Spouses == 0):\n",
    "            return prediction(tree.left, instance)\n",
    "        else:\n",
    "            return prediction(tree.right, instance)\n",
    "            \n",
    "    elif(tree.val == 5):\n",
    "        if(instance.parents_Children):\n",
    "            return prediction(tree.left, instance)\n",
    "        else:\n",
    "            return prediction(tree.right, instance)\n",
    "    \n",
    "    elif(tree.val == 6):\n",
    "        if(instance.fare == 0):\n",
    "            return prediction(tree.left, instance)\n",
    "        else:\n",
    "            return prediction(tree.right, instance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "random_forest_sample is a function that builds a random forest with 5 decision trees, using a random subset\n",
    "of 80% of the samples for each tree.\n",
    "\n",
    "Input: 1) instances- the list of training instances\n",
    "\n",
    "Output: a random forest with 5 decision trees\n",
    "'''\n",
    "def random_forest_sample(instances):\n",
    "    \n",
    "    forest = []\n",
    "    splits_available = [1,2,3,4,5,6]\n",
    "    \n",
    "    for i in range(5):\n",
    "        \n",
    "        #Generate non-duplicate random indices for 80% of the samples\n",
    "        temp = random.sample(range(len(instances)), round(0.8 * len(instances)))\n",
    "        \n",
    "        temp_instances = []\n",
    "        \n",
    "        #Use the random indices and append selected indices\n",
    "        for elem in temp:\n",
    "            temp_instances.append(instances[elem])\n",
    "        \n",
    "        \n",
    "        #Call the decision tree algorithm from Question 3\n",
    "        temp_tree = decision_tree_alg(temp_instances, splits_available, len(temp_instances))\n",
    "        forest.append(temp_tree)\n",
    "    \n",
    "    return forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "random_forest_cross_validation_samples is a function that performs 10-fold cross-validation for the random forest\n",
    "\n",
    "Input: 1) instances - the list of instances on which we will perform CV\n",
    "\n",
    "Output: the average accuracy of 10-fold cross validation\n",
    "'''\n",
    "def random_forest_cross_validation_samples(instances):\n",
    "    \n",
    "    #Divide into 10 subsets \n",
    "    subset_1 = []\n",
    "    subset_2 = []\n",
    "    subset_3 = []\n",
    "    subset_4 = []\n",
    "    subset_5 = []\n",
    "    subset_6 = []\n",
    "    subset_7 = []\n",
    "    subset_8 = []\n",
    "    subset_9 = []\n",
    "    subset_10 = []\n",
    "    \n",
    "    #Since there are 887 samples we'll have 9 subsets that have 89 samples and 1 that has 86\n",
    "    \n",
    "    temp = []\n",
    "    \n",
    "    #call balance_classes method defined previously\n",
    "    balanced_dist = balance_classes(instances)\n",
    "    \n",
    "    \n",
    "    \n",
    "    for i in range(len(balanced_dist)):\n",
    "        \n",
    "        if(i < 89):\n",
    "            subset_1.append(balanced_dist[i])\n",
    "        elif(i < 178):\n",
    "            subset_2.append(balanced_dist[i])\n",
    "        elif(i < 267):\n",
    "            subset_3.append(balanced_dist[i])\n",
    "        elif(i < 356):\n",
    "            subset_4.append(balanced_dist[i])\n",
    "        elif(i < 445):\n",
    "            subset_5.append(balanced_dist[i])\n",
    "        elif(i < 534):\n",
    "            subset_6.append(balanced_dist[i])\n",
    "        elif(i < 623):\n",
    "            subset_7.append(balanced_dist[i])\n",
    "        elif(i < 712):\n",
    "            subset_8.append(balanced_dist[i])\n",
    "        elif(i < 801):\n",
    "            subset_9.append(balanced_dist[i])\n",
    "        else:\n",
    "            subset_10.append(balanced_dist[i])\n",
    "    \n",
    "    \n",
    "    for element in balanced_dist:\n",
    "        temp.append(element)\n",
    "        \n",
    "    #drop subset 1\n",
    "    for i in range(89):\n",
    "        del temp[0]\n",
    "    \n",
    "    #Generate random forest using subsets 2-10 as training\n",
    "    forest1 = random_forest_sample(temp)\n",
    "    \n",
    "    #Compute accuracy using subset 1 as testing (compute_accuracy_Forest is defined in next cell )\n",
    "    accuracy1 = compute_accuracy_Forest(forest1, subset_1)\n",
    "    print(accuracy1)\n",
    "    \n",
    "    \n",
    "    temp = []\n",
    "    for element in balanced_dist:\n",
    "        temp.append(element)\n",
    "    \n",
    "    #drop subset 2\n",
    "    for i in range(89):\n",
    "        del temp[89]\n",
    "        \n",
    "    #Generate random forest using subset 1 and 3-10 as training\n",
    "    forest2 = random_forest_sample(temp)\n",
    "    \n",
    "    #Compute accuracy using subset 2 as testing\n",
    "    accuracy2 = compute_accuracy_Forest(forest2, subset_2)\n",
    "    print(accuracy2)\n",
    "    \n",
    "    \n",
    "    temp = []\n",
    "    for element in balanced_dist:\n",
    "        temp.append(element)\n",
    "    \n",
    "    #drop subset 3\n",
    "    for i in range(89):\n",
    "        del temp[178]\n",
    "        \n",
    "    #Generate random forest using subsets 1-2 and 4-10 as training\n",
    "    forest3 = random_forest_sample(temp)\n",
    "    \n",
    "    #Compute accuracy using subset 3 as testing\n",
    "    accuracy3 = compute_accuracy_Forest(forest3, subset_3)\n",
    "    print(accuracy3)\n",
    "    \n",
    "    \n",
    "    temp = []\n",
    "    for element in balanced_dist:\n",
    "        temp.append(element)\n",
    "   \n",
    "    #drop subset 4\n",
    "    for i in range(89):\n",
    "        del temp[267]\n",
    "    \n",
    "    #Generate random forest using subsets 1-3 and 5-10 as training\n",
    "    forest4 = random_forest_sample(temp)\n",
    "    \n",
    "    #Compute accuracy using subset 4 as testing\n",
    "    accuracy4 = compute_accuracy_Forest(forest4, subset_4)\n",
    "    print(accuracy4)\n",
    "    \n",
    "    \n",
    "    temp = []\n",
    "    for element in balanced_dist:\n",
    "        temp.append(element)\n",
    "    \n",
    "    #drop subset 5\n",
    "    for i in range(89):\n",
    "        del temp[356]\n",
    "        \n",
    "    #Generate random forest using subsets 1-4 and 6-10 as training\n",
    "    forest5 = random_forest_sample(temp)\n",
    "    \n",
    "    #Compute accuracy using subset 5 as testing\n",
    "    accuracy5 = compute_accuracy_Forest(forest5, subset_5)\n",
    "    print(accuracy5)\n",
    "    \n",
    "    \n",
    "    temp = []\n",
    "    for element in balanced_dist:\n",
    "        temp.append(element)\n",
    "        \n",
    "    #drop subset 6\n",
    "    for i in range(89):\n",
    "        del temp[445]\n",
    "        \n",
    "    #Generate random forest using subsets 1-5 and 7-10 as training\n",
    "    forest6 = random_forest_sample(temp)\n",
    "    \n",
    "    #Compute accuracy using subset 6 as testing\n",
    "    accuracy6 = compute_accuracy_Forest(forest6, subset_6)\n",
    "    print(accuracy6)\n",
    "    \n",
    "    \n",
    "    temp = []\n",
    "    for element in balanced_dist:\n",
    "        temp.append(element)\n",
    "    \n",
    "    #drop subset 7\n",
    "    for i in range(89):\n",
    "        del temp[534]\n",
    "    \n",
    "    #Generate random forest using subsets 1-6 and 8-10 as training\n",
    "    forest7 = random_forest_sample(temp)\n",
    "    \n",
    "    #Compute accuracy using subset 7 as testing\n",
    "    accuracy7 = compute_accuracy_Forest(forest7, subset_7)\n",
    "    print(accuracy7)\n",
    "    \n",
    "    \n",
    "    temp = []\n",
    "    for element in balanced_dist:\n",
    "        temp.append(element)\n",
    "    \n",
    "    #drop subset 8\n",
    "    for i in range(89):\n",
    "        del temp[623]\n",
    "    \n",
    "    #Generate random forest using subsets 1-7 and 9-10 as training\n",
    "    forest8 = random_forest_sample(temp)\n",
    "    \n",
    "    #Compute accuracy using subset 8 as testing\n",
    "    accuracy8 = compute_accuracy_Forest(forest8, subset_8)\n",
    "    print(accuracy8)\n",
    "    \n",
    "    \n",
    "    temp = []\n",
    "    for element in balanced_dist:\n",
    "        temp.append(element)\n",
    "        \n",
    "    #drop subset 9\n",
    "    for i in range(89):\n",
    "        del temp[712]\n",
    "        \n",
    "    #Generate random forest using subsets 1-8 and 10 as training\n",
    "    forest9 = random_forest_sample(temp)\n",
    "    \n",
    "    #Compute accuracy using subset 9 as testing\n",
    "    accuracy9 = compute_accuracy_Forest(forest9, subset_9)\n",
    "    print(accuracy9)\n",
    "    \n",
    "    \n",
    "    temp = []\n",
    "    for element in balanced_dist:\n",
    "        temp.append(element)\n",
    "    \n",
    "    #drop subset 10\n",
    "    for i in range(86):\n",
    "        del temp[801]\n",
    "    \n",
    "    #Generate random forest using subsets 1-9 as training\n",
    "    forest10 = random_forest_sample(temp)\n",
    "    \n",
    "    #Compute accuracy using subset 10 as testing\n",
    "    accuracy10 = compute_accuracy_Forest(forest10, subset_10)\n",
    "    print(accuracy10)\n",
    "    \n",
    "    #Compute total accuracy\n",
    "    total_accuracy = (accuracy1 + accuracy2 + accuracy3 + accuracy4 + accuracy5 + accuracy6 + accuracy7 + accuracy8 + accuracy9 + accuracy10)/10\n",
    "    print('Total Accuracy: ' + str(total_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "compute_accuracy_Forest is a function that can be used to determine the random forest's prediction\n",
    "        accuracy for a subset of test samples\n",
    "\n",
    "Input: 1) forest: the random forest for which we are calculating accuracy\n",
    "\n",
    "       2) subset: the subset of test instances we'll use to calculate accuracy\n",
    "'''\n",
    "def compute_accuracy_Forest(forest, subset):\n",
    "    \n",
    "    num_correct = 0\n",
    "    \n",
    "    for elem in subset:\n",
    "        \n",
    "        #forest_prediction method is defined below\n",
    "        pred = forest_prediction(forest, elem)\n",
    "        \n",
    "        if(pred == elem.survived):\n",
    "            num_correct+=1\n",
    "    \n",
    "    accuracy = float(num_correct)/len(subset)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "forest_prediction is a method to determine the random forest's prediction for a test instance\n",
    "\n",
    "Input: 1) forest - the random forest we'll use to determine prediction\n",
    "\n",
    "       2) instance - the test instance for which we want a prediction\n",
    "\n",
    "\n",
    "Output: the random forest's prediction\n",
    "'''\n",
    "def forest_prediction(forest, instance):\n",
    "    \n",
    "    num_zero = 0\n",
    "    num_one = 0\n",
    "    \n",
    "    for tree in forest:\n",
    "        #the prediction function is the one we used in question 5 for trees\n",
    "        predict = prediction(tree, instance)\n",
    "        if(predict == 0):\n",
    "            num_zero+=1\n",
    "        else:\n",
    "            num_one+=1\n",
    "    \n",
    "    if(num_zero >= num_one):\n",
    "        return 0\n",
    "    else:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "random_forest_feature is a function that builds a random forest with 6 decision trees, each excluding one of\n",
    "the features in the dataset.\n",
    "\n",
    "Input: 1) instances - the list of training instances\n",
    "\n",
    "Output: a random forest with 6 decision trees\n",
    "'''\n",
    "def random_forest_feature(instances):\n",
    "    \n",
    "    forest = []\n",
    "    \n",
    "    #Leave feature 1 out\n",
    "    splits_1 = [2,3,4,5,6]\n",
    "    tree_1 = decision_tree_alg(instances, splits_1, len(instances))\n",
    "    forest.append(tree_1)\n",
    "    \n",
    "    #Leave feature 2 out\n",
    "    splits_2 = [1,3,4,5,6]\n",
    "    tree_2 = decision_tree_alg(instances, splits_2, len(instances))\n",
    "    forest.append(tree_2)\n",
    "    \n",
    "    #Leave feature 3 out\n",
    "    splits_3 = [1,2,4,5,6]\n",
    "    tree_3 = decision_tree_alg(instances, splits_3, len(instances))\n",
    "    forest.append(tree_3)\n",
    "    \n",
    "    #Leave feature 4 out\n",
    "    splits_4 = [1,2,3,5,6]\n",
    "    tree_4 = decision_tree_alg(instances, splits_4, len(instances))\n",
    "    forest.append(tree_4)\n",
    "    \n",
    "    #Leave feature 5 out\n",
    "    splits_5 = [1,2,3,4,6]\n",
    "    tree_5 = decision_tree_alg(instances, splits_5, len(instances))\n",
    "    forest.append(tree_5)\n",
    "    \n",
    "    #Leave feature 6 out\n",
    "    splits_6 = [1,2,3,4,5]\n",
    "    tree_6 = decision_tree_alg(instances, splits_6, len(instances))\n",
    "    forest.append(tree_6)\n",
    "    \n",
    "    return forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "random_forest_cross_validation_features is a function that performs 10-fold cross-validation for the random forest\n",
    "\n",
    "Input: 1) instances - the list of instances on which we will perform CV\n",
    "\n",
    "Output: the average accuracy of 10-fold cross validation\n",
    "'''\n",
    "def random_forest_cross_validation_features(instances):\n",
    "    \n",
    "    #Divide into 10 subsets \n",
    "    subset_1 = []\n",
    "    subset_2 = []\n",
    "    subset_3 = []\n",
    "    subset_4 = []\n",
    "    subset_5 = []\n",
    "    subset_6 = []\n",
    "    subset_7 = []\n",
    "    subset_8 = []\n",
    "    subset_9 = []\n",
    "    subset_10 = []\n",
    "    \n",
    "    #Since there are 887 samples we'll have 9 subsets that have 89 samples and 1 that has 86\n",
    "    \n",
    "    temp = []\n",
    "    \n",
    "    #call balance_classes method defined previously\n",
    "    balanced_dist = balance_classes(instances)\n",
    "    \n",
    "    \n",
    "    \n",
    "    for i in range(len(balanced_dist)):\n",
    "        \n",
    "        if(i < 89):\n",
    "            subset_1.append(balanced_dist[i])\n",
    "        elif(i < 178):\n",
    "            subset_2.append(balanced_dist[i])\n",
    "        elif(i < 267):\n",
    "            subset_3.append(balanced_dist[i])\n",
    "        elif(i < 356):\n",
    "            subset_4.append(balanced_dist[i])\n",
    "        elif(i < 445):\n",
    "            subset_5.append(balanced_dist[i])\n",
    "        elif(i < 534):\n",
    "            subset_6.append(balanced_dist[i])\n",
    "        elif(i < 623):\n",
    "            subset_7.append(balanced_dist[i])\n",
    "        elif(i < 712):\n",
    "            subset_8.append(balanced_dist[i])\n",
    "        elif(i < 801):\n",
    "            subset_9.append(balanced_dist[i])\n",
    "        else:\n",
    "            subset_10.append(balanced_dist[i])\n",
    "    \n",
    "    for element in balanced_dist:\n",
    "        temp.append(element)\n",
    "    \n",
    "    #drop subset 1\n",
    "    for i in range(89):\n",
    "        del temp[0]\n",
    "    \n",
    "    #Generate random forest using subsets 2-10 as training\n",
    "    forest1 = random_forest_feature(temp)\n",
    "    \n",
    "    #Compute accuracy using subset 1 as testing \n",
    "    accuracy1 = compute_accuracy_Forest(forest1, subset_1)\n",
    "    print(accuracy1)\n",
    "    \n",
    "    \n",
    "    temp = []\n",
    "    for element in balanced_dist:\n",
    "        temp.append(element)\n",
    "    \n",
    "    #drop subset 2\n",
    "    for i in range(89):\n",
    "        del temp[89]\n",
    "        \n",
    "    #Generate random forest using subsets 1 and 3-10 as training\n",
    "    forest2 = random_forest_feature(temp)\n",
    "    \n",
    "    #Compute accuracy using subset 2 as testing\n",
    "    accuracy2 = compute_accuracy_Forest(forest2, subset_2)\n",
    "    print(accuracy2)\n",
    "    \n",
    "    \n",
    "    temp = []\n",
    "    for element in balanced_dist:\n",
    "        temp.append(element)\n",
    "    \n",
    "    #drop subset 3\n",
    "    for i in range(89):\n",
    "        del temp[178]\n",
    "        \n",
    "    #Generate random forest using subsets 1-2 and 4-10 as training\n",
    "    forest3 = random_forest_feature(temp)\n",
    "    \n",
    "    #Compute accuracy using subset 3 as testing\n",
    "    accuracy3 = compute_accuracy_Forest(forest3, subset_3)\n",
    "    print(accuracy3)\n",
    "    \n",
    "    \n",
    "    temp = []\n",
    "    for element in balanced_dist:\n",
    "        temp.append(element)\n",
    "   \n",
    "    #drop subset 4\n",
    "    for i in range(89):\n",
    "        del temp[267]\n",
    "    \n",
    "    #Generate random forest using subsets 1-3 and 5-10 as training\n",
    "    forest4 = random_forest_feature(temp)\n",
    "    \n",
    "    #Compute accuracy using subset 4 as testing\n",
    "    accuracy4 = compute_accuracy_Forest(forest4, subset_4)\n",
    "    print(accuracy4)\n",
    "    \n",
    "    \n",
    "    temp = []\n",
    "    for element in balanced_dist:\n",
    "        temp.append(element)\n",
    "    \n",
    "    #drop subset 5\n",
    "    for i in range(89):\n",
    "        del temp[356]\n",
    "        \n",
    "    #Generate random forest using subsets 1-4 and 6-10 as training\n",
    "    forest5 = random_forest_feature(temp)\n",
    "    \n",
    "    #Compute accuracy using subset 5 as testing\n",
    "    accuracy5 = compute_accuracy_Forest(forest5, subset_5)\n",
    "    print(accuracy5)\n",
    "    \n",
    "    \n",
    "    temp = []\n",
    "    for element in balanced_dist:\n",
    "        temp.append(element)\n",
    "        \n",
    "    #drop subset 6\n",
    "    for i in range(89):\n",
    "        del temp[445]\n",
    "        \n",
    "    #Generate random forest using subsets 1-5 and 7-10 as training\n",
    "    forest6 = random_forest_feature(temp)\n",
    "    \n",
    "    #Compute accuracy using subset 6 as testing\n",
    "    accuracy6 = compute_accuracy_Forest(forest6, subset_6)\n",
    "    print(accuracy6)\n",
    "    \n",
    "    \n",
    "    temp = []\n",
    "    for element in balanced_dist:\n",
    "        temp.append(element)\n",
    "    \n",
    "    #drop subset 7\n",
    "    for i in range(89):\n",
    "        del temp[534]\n",
    "    \n",
    "    #Generate random forest using subsets 1-6 and 8-10 as training\n",
    "    forest7 = random_forest_feature(temp)\n",
    "    \n",
    "    #Compute accuracy using subset 7 as testing\n",
    "    accuracy7 = compute_accuracy_Forest(forest7, subset_7)\n",
    "    print(accuracy7)\n",
    "    \n",
    "    \n",
    "    temp = []\n",
    "    for element in balanced_dist:\n",
    "        temp.append(element)\n",
    "    \n",
    "    #drop subset 8\n",
    "    for i in range(89):\n",
    "        del temp[623]\n",
    "    \n",
    "    #Generate random forest using subsets 1-7 and 9-10 as training\n",
    "    forest8 = random_forest_feature(temp)\n",
    "    \n",
    "    #Compute accuracy using subset 8 as testing\n",
    "    accuracy8 = compute_accuracy_Forest(forest8, subset_8)\n",
    "    print(accuracy8)\n",
    "    \n",
    "    \n",
    "    temp = []\n",
    "    for element in balanced_dist:\n",
    "        temp.append(element)\n",
    "        \n",
    "    #drop subset 9\n",
    "    for i in range(89):\n",
    "        del temp[712]\n",
    "        \n",
    "    #Generate random forest using subsets 1-8 and 10 as training\n",
    "    forest9 = random_forest_feature(temp)\n",
    "    \n",
    "    #Compute accuracy using subset 9 as testing\n",
    "    accuracy9 = compute_accuracy_Forest(forest9, subset_9)\n",
    "    print(accuracy9)\n",
    "    \n",
    "    \n",
    "    temp = []\n",
    "    for element in balanced_dist:\n",
    "        temp.append(element)\n",
    "    \n",
    "    #drop subset 10\n",
    "    for i in range(86):\n",
    "        del temp[801]\n",
    "    \n",
    "    #Generate random forest using subsets 1-9 as training\n",
    "    forest10 = random_forest_feature(temp)\n",
    "    \n",
    "    #Compute accuracy using subset 10 as testing\n",
    "    accuracy10 = compute_accuracy_Forest(forest10, subset_10)\n",
    "    print(accuracy10)\n",
    "    \n",
    "    #Compute average accuracy\n",
    "    total_accuracy = (accuracy1 + accuracy2 + accuracy3 + accuracy4 + accuracy5 + accuracy6 + accuracy7 + accuracy8 + accuracy9 + accuracy10)/10\n",
    "    print('Total Accuracy: ' + str(total_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Code to load dataset\n",
    "'''\n",
    "\n",
    "import pandas as pd\n",
    "import math\n",
    "import random \n",
    "\n",
    "df = pd.read_csv('titanic_data.csv')\n",
    "df.rename(columns = {\"Siblings/Spouses Aboard\": \"Siblings_Spouses_Aboard\"}, inplace = True)\n",
    "df.rename(columns = {\"Parents/Children Aboard\": \"Parents_Children_Aboard\"}, inplace = True)\n",
    "df.loc[df.Age < 28, \"Age\"] = 0\n",
    "df.loc[df.Age >= 28, \"Age\"] = 1\n",
    "df.loc[df.Pclass < 3, \"Pclass\"] = 0\n",
    "df.loc[df.Pclass >=3, \"Pclass\"] = 1\n",
    "df.loc[df.Siblings_Spouses_Aboard <= 0, \"Siblings_Spouses_Aboard\"] = 0\n",
    "df.loc[df.Siblings_Spouses_Aboard > 0, \"Siblings_Spouses_Aboard\"] = 1\n",
    "df.loc[df.Parents_Children_Aboard <= 0, \"Parents_Children_Aboard\"] = 0\n",
    "df.loc[df.Parents_Children_Aboard > 0, \"Parents_Children_Aboard\"] = 1\n",
    "df.loc[df.Fare < 14.45, \"Fare\"] = 0\n",
    "df.loc[df.Fare >= 14.45, \"Fare\"] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>Siblings_Spouses_Aboard</th>\n",
       "      <th>Parents_Children_Aboard</th>\n",
       "      <th>Fare</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>882</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>883</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>884</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>885</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>886</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>887 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Survived  Pclass  Sex  Age  Siblings_Spouses_Aboard  \\\n",
       "0           0       1    0  0.0                        1   \n",
       "1           1       0    1  1.0                        1   \n",
       "2           1       1    1  0.0                        0   \n",
       "3           1       0    1  1.0                        1   \n",
       "4           0       1    0  1.0                        0   \n",
       "..        ...     ...  ...  ...                      ...   \n",
       "882         0       0    0  0.0                        0   \n",
       "883         1       0    1  0.0                        0   \n",
       "884         0       1    1  0.0                        1   \n",
       "885         1       0    0  0.0                        0   \n",
       "886         0       1    0  1.0                        0   \n",
       "\n",
       "     Parents_Children_Aboard  Fare  \n",
       "0                          0   0.0  \n",
       "1                          0   1.0  \n",
       "2                          0   0.0  \n",
       "3                          0   1.0  \n",
       "4                          0   0.0  \n",
       "..                       ...   ...  \n",
       "882                        0   0.0  \n",
       "883                        0   1.0  \n",
       "884                        1   1.0  \n",
       "885                        0   1.0  \n",
       "886                        0   0.0  \n",
       "\n",
       "[887 rows x 7 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Code to generate training instances\n",
    "'''\n",
    "\n",
    "train_instance = []\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    \n",
    "    temp = TrainingInstance(row['Pclass'], row['Sex'], row['Age'], row['Siblings_Spouses_Aboard'], row['Parents_Children_Aboard'], row['Fare'], row['Survived'])\n",
    "    train_instance.append(temp)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Level 0\n",
      "\n",
      "[2]\n",
      "\n",
      "Level 1\n",
      "\n",
      "[6, 1]\n",
      "\n",
      "Level 2\n",
      "\n",
      "[5, 1, 6, 6]\n",
      "\n",
      "Level 3\n",
      "\n",
      "[4, 'Deceased', 3, 4, 'Survived', 4, 3, 5]\n",
      "\n",
      "Level 4\n",
      "\n",
      "[3, 'Deceased', 'Survived', 5, 'Deceased', 3, 5, 3, 5, 'Deceased', 'Survived', 4]\n",
      "\n",
      "Level 5\n",
      "\n",
      "[1, 1, 4, 'Deceased', 5, 'Deceased', 'Survived', 'Survived', 'Survived', 5, 4, 'Survived', 'Deceased', 'Deceased']\n",
      "\n",
      "Level 6\n",
      "\n",
      "['Deceased', 'Deceased', 'Deceased', 'Deceased', 'Deceased', 'Deceased', 'Deceased', 'Deceased', 'Survived', 'Survived', 'Survived', 'Deceased']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Generate Decision Tree and display (Problem 4.4)\n",
    "\n",
    "splits_available = [1,2,3,4,5,6]\n",
    "tree = decision_tree_alg(train_instance, splits_available, len(train_instance))\n",
    "\n",
    "lists = []\n",
    "level_order_Traversal(tree, 0, lists)\n",
    "for i in range(len(lists)):\n",
    "    print(\"Level \" + str(i) )\n",
    "    print()\n",
    "    print(lists[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7640449438202247\n",
      "0.7528089887640449\n",
      "0.7640449438202247\n",
      "0.8426966292134831\n",
      "0.7865168539325843\n",
      "0.7640449438202247\n",
      "0.8314606741573034\n",
      "0.7752808988764045\n",
      "0.7865168539325843\n",
      "0.8023255813953488\n",
      "total accuracy: 0.7869741311732427\n"
     ]
    }
   ],
   "source": [
    "#Computing Cross Validation (Problem 4.5)\n",
    "cross_validation(train_instance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "#Computing prediction for feature vector (Problem 4.6)\n",
    "my_feature_vector = TestInstance(0, 0, 0, 0, 0, 1)\n",
    "pred = prediction(tree, my_feature_vector)\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tree 1\n",
      "\n",
      "Level 0\n",
      "[2]\n",
      "\n",
      "Level 1\n",
      "[6, 1]\n",
      "\n",
      "Level 2\n",
      "[5, 1, 6, 5]\n",
      "\n",
      "Level 3\n",
      "[4, 'Deceased', 3, 4, 'Survived', 3, 3, 6]\n",
      "\n",
      "Level 4\n",
      "[1, 'Deceased', 'Survived', 5, 'Deceased', 3, 5, 4, 4, 'Survived', 'Survived', 'Deceased']\n",
      "\n",
      "Level 5\n",
      "[3, 3, 4, 'Deceased', 5, 'Deceased', 'Survived', 'Survived', 5, 'Survived', 6, 'Deceased']\n",
      "\n",
      "Level 6\n",
      "['Deceased', 'Deceased', 'Deceased', 'Deceased', 'Deceased', 'Deceased', 'Deceased', 'Deceased', 'Survived', 'Survived', 'Survived', 'Deceased']\n",
      "\n",
      "\n",
      "\n",
      "Tree 2\n",
      "\n",
      "Level 0\n",
      "[2]\n",
      "\n",
      "Level 1\n",
      "[6, 1]\n",
      "\n",
      "Level 2\n",
      "[5, 1, 3, 5]\n",
      "\n",
      "Level 3\n",
      "[4, 'Deceased', 3, 4, 4, 5, 4, 6]\n",
      "\n",
      "Level 4\n",
      "[1, 'Deceased', 'Survived', 5, 'Deceased', 3, 'Survived', 'Survived', 6, 'Survived', 3, 'Survived', 'Survived', 4]\n",
      "\n",
      "Level 5\n",
      "[3, 3, 4, 'Deceased', 5, 'Deceased', 'Survived', 4, 'Survived', 'Deceased', 'Deceased', 'Deceased']\n",
      "\n",
      "Level 6\n",
      "['Deceased', 'Deceased', 'Deceased', 'Deceased', 'Deceased', 'Deceased', 'Deceased', 'Deceased', 'Survived', 'Survived']\n",
      "\n",
      "\n",
      "\n",
      "Tree 3\n",
      "\n",
      "Level 0\n",
      "[2]\n",
      "\n",
      "Level 1\n",
      "[6, 1]\n",
      "\n",
      "Level 2\n",
      "[4, 1, 6, 5]\n",
      "\n",
      "Level 3\n",
      "[5, 'Deceased', 3, 4, 'Survived', 3, 4, 3]\n",
      "\n",
      "Level 4\n",
      "[1, 'Deceased', 'Survived', 5, 'Deceased', 3, 4, 4, 6, 'Survived', 'Deceased', 'Deceased']\n",
      "\n",
      "Level 5\n",
      "[3, 3, 4, 'Deceased', 5, 'Deceased', 'Survived', 'Survived', 'Survived', 'Survived', 3, 'Deceased']\n",
      "\n",
      "Level 6\n",
      "['Deceased', 'Deceased', 'Deceased', 'Deceased', 'Deceased', 'Deceased', 'Deceased', 'Deceased', 'Survived', 'Deceased']\n",
      "\n",
      "\n",
      "\n",
      "Tree 4\n",
      "\n",
      "Level 0\n",
      "[2]\n",
      "\n",
      "Level 1\n",
      "[6, 1]\n",
      "\n",
      "Level 2\n",
      "[1, 1, 6, 6]\n",
      "\n",
      "Level 3\n",
      "[3, 5, 3, 4, 'Survived', 4, 3, 5]\n",
      "\n",
      "Level 4\n",
      "['Deceased', 'Deceased', 4, 'Deceased', 'Survived', 5, 'Deceased', 3, 5, 3, 5, 'Deceased', 'Survived', 4]\n",
      "\n",
      "Level 5\n",
      "[3, 'Deceased', 4, 'Deceased', 5, 'Deceased', 'Survived', 'Survived', 'Survived', 'Survived', 4, 'Survived', 'Deceased', 'Deceased']\n",
      "\n",
      "Level 6\n",
      "['Deceased', 'Deceased', 'Deceased', 'Deceased', 'Deceased', 'Deceased', 'Survived', 'Deceased']\n",
      "\n",
      "\n",
      "\n",
      "Tree 5\n",
      "\n",
      "Level 0\n",
      "[2]\n",
      "\n",
      "Level 1\n",
      "[6, 1]\n",
      "\n",
      "Level 2\n",
      "[5, 1, 3, 6]\n",
      "\n",
      "Level 3\n",
      "[4, 'Deceased', 3, 4, 4, 5, 3, 3]\n",
      "\n",
      "Level 4\n",
      "[3, 'Deceased', 'Survived', 5, 'Deceased', 3, 'Survived', 'Survived', 6, 'Survived', 5, 'Deceased', 4, 'Deceased']\n",
      "\n",
      "Level 5\n",
      "[1, 1, 4, 'Deceased', 'Deceased', 'Deceased', 'Survived', 4, 4, 'Survived', 'Survived', 'Deceased']\n",
      "\n",
      "Level 6\n",
      "['Deceased', 'Deceased', 'Deceased', 'Deceased', 'Deceased', 'Deceased', 'Survived', 'Survived', 'Survived', 'Deceased']\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Generating random forest and display (Problem 4.7 a)\n",
    "forest1 = random_forest_sample(train_instance)\n",
    "\n",
    "j = 1\n",
    "for tree in forest1:\n",
    "        \n",
    "    lists = []\n",
    "    level_order_Traversal(tree, 0, lists)\n",
    "    print(\"Tree \" + str(j) )\n",
    "    print()\n",
    "    for i in range(len(lists)):\n",
    "        print(\"Level \" + str(i))\n",
    "        print(lists[i])\n",
    "        print()\n",
    "    \n",
    "    print()\n",
    "    j += 1\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8314606741573034\n",
      "0.7640449438202247\n",
      "0.7528089887640449\n",
      "0.8426966292134831\n",
      "0.7865168539325843\n",
      "0.7865168539325843\n",
      "0.8202247191011236\n",
      "0.7415730337078652\n",
      "0.7865168539325843\n",
      "0.8023255813953488\n",
      "Total Accuracy: 0.7914685131957147\n"
     ]
    }
   ],
   "source": [
    "#Computing Cross Validation (Problem 4.7 b)\n",
    "random_forest_cross_validation_samples(train_instance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "#Computing prediction for feature vector (Problem 4.7 c)\n",
    "my_feature_vector = TestInstance(0, 0, 0, 0, 0, 1)\n",
    "pred = forest_prediction(forest1, my_feature_vector)\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tree 1\n",
      "\n",
      "Level 0\n",
      "[2]\n",
      "\n",
      "Level 1\n",
      "[6, 5]\n",
      "\n",
      "Level 2\n",
      "[5, 3, 6, 4]\n",
      "\n",
      "Level 3\n",
      "[4, 'Deceased', 5, 5, 4, 3, 3, 3]\n",
      "\n",
      "Level 4\n",
      "[3, 'Deceased', 'Deceased', 4, 4, 'Deceased', 3, 'Deceased', 'Survived', 4, 'Survived', 'Survived', 6, 'Survived']\n",
      "\n",
      "Level 5\n",
      "['Deceased', 'Deceased', 'Survived', 'Deceased', 'Deceased', 'Deceased', 'Survived', 'Survived', 'Survived', 'Survived', 'Survived', 'Deceased']\n",
      "\n",
      "\n",
      "\n",
      "Tree 2\n",
      "\n",
      "Level 0\n",
      "[1]\n",
      "\n",
      "Level 1\n",
      "[6, 3]\n",
      "\n",
      "Level 2\n",
      "[3, 3, 5, 6]\n",
      "\n",
      "Level 3\n",
      "['Deceased', 'Deceased', 5, 4, 4, 4, 4, 5]\n",
      "\n",
      "Level 4\n",
      "['Survived', 4, 5, 5, 6, 'Deceased', 'Survived', 6, 5, 'Deceased', 'Deceased', 'Deceased']\n",
      "\n",
      "Level 5\n",
      "['Survived', 'Survived', 'Survived', 'Survived', 'Survived', 'Survived', 'Deceased', 'Deceased', 'Deceased', 'Deceased', 'Deceased', 'Deceased']\n",
      "\n",
      "\n",
      "\n",
      "Tree 3\n",
      "\n",
      "Level 0\n",
      "[2]\n",
      "\n",
      "Level 1\n",
      "[6, 1]\n",
      "\n",
      "Level 2\n",
      "[5, 1, 6, 6]\n",
      "\n",
      "Level 3\n",
      "[4, 'Deceased', 5, 4, 'Survived', 4, 4, 5]\n",
      "\n",
      "Level 4\n",
      "[1, 'Deceased', 4, 'Deceased', 'Deceased', 5, 5, 5, 5, 'Deceased', 'Survived', 4]\n",
      "\n",
      "Level 5\n",
      "['Deceased', 'Deceased', 'Deceased', 'Deceased', 'Deceased', 'Deceased', 'Survived', 'Survived', 'Survived', 'Survived', 'Survived', 'Survived', 'Deceased', 'Deceased']\n",
      "\n",
      "\n",
      "\n",
      "Tree 4\n",
      "\n",
      "Level 0\n",
      "[2]\n",
      "\n",
      "Level 1\n",
      "[6, 1]\n",
      "\n",
      "Level 2\n",
      "[5, 1, 6, 6]\n",
      "\n",
      "Level 3\n",
      "[1, 'Deceased', 3, 3, 'Survived', 3, 3, 5]\n",
      "\n",
      "Level 4\n",
      "[3, 3, 'Survived', 5, 5, 'Deceased', 5, 5, 5, 'Deceased', 'Survived', 3]\n",
      "\n",
      "Level 5\n",
      "['Deceased', 'Deceased', 'Deceased', 'Deceased', 'Deceased', 'Deceased', 'Deceased', 'Deceased', 'Survived', 'Survived', 'Survived', 'Survived', 'Survived', 'Survived', 'Deceased', 'Deceased']\n",
      "\n",
      "\n",
      "\n",
      "Tree 5\n",
      "\n",
      "Level 0\n",
      "[2]\n",
      "\n",
      "Level 1\n",
      "[6, 1]\n",
      "\n",
      "Level 2\n",
      "[4, 1, 6, 6]\n",
      "\n",
      "Level 3\n",
      "[1, 'Deceased', 3, 4, 'Survived', 4, 3, 3]\n",
      "\n",
      "Level 4\n",
      "[3, 3, 'Survived', 4, 'Deceased', 3, 3, 3, 4, 'Deceased', 'Deceased', 'Deceased']\n",
      "\n",
      "Level 5\n",
      "['Deceased', 'Deceased', 'Deceased', 'Deceased', 'Deceased', 'Deceased', 'Deceased', 'Deceased', 'Survived', 'Survived', 'Survived', 'Survived', 'Survived', 'Survived']\n",
      "\n",
      "\n",
      "\n",
      "Tree 6\n",
      "\n",
      "Level 0\n",
      "[2]\n",
      "\n",
      "Level 1\n",
      "[1, 1]\n",
      "\n",
      "Level 2\n",
      "[5, 5, 3, 5]\n",
      "\n",
      "Level 3\n",
      "[4, 'Deceased', 3, 3, 4, 5, 3, 3]\n",
      "\n",
      "Level 4\n",
      "[3, 'Deceased', 4, 4, 'Deceased', 'Deceased', 'Survived', 'Survived', 4, 'Survived', 4, 'Survived', 'Deceased', 'Deceased']\n",
      "\n",
      "Level 5\n",
      "['Deceased', 'Deceased', 'Deceased', 'Deceased', 'Deceased', 'Deceased', 'Survived', 'Survived', 'Survived', 'Deceased']\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Generating random forest and display (Problem 4.8 a)\n",
    "forest2 = random_forest_feature(train_instance)\n",
    "j = 1\n",
    "for tree in forest2:\n",
    "    \n",
    "    lists = []\n",
    "    level_order_Traversal(tree, 0, lists)\n",
    "    \n",
    "    print('Tree ' + str(j))\n",
    "    print()\n",
    "    \n",
    "    for i in range(len(lists)):\n",
    "        print(\"Level \" + str(i))\n",
    "        print(lists[i])\n",
    "        print()\n",
    "    \n",
    "    print()  \n",
    "    j += 1\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7528089887640449\n",
      "0.7865168539325843\n",
      "0.7528089887640449\n",
      "0.8426966292134831\n",
      "0.8202247191011236\n",
      "0.7752808988764045\n",
      "0.8314606741573034\n",
      "0.8089887640449438\n",
      "0.797752808988764\n",
      "0.7906976744186046\n",
      "Total Accuracy: 0.7959237000261301\n"
     ]
    }
   ],
   "source": [
    "#Computing Cross Validation (Problem 4.8 b)\n",
    "random_forest_cross_validation_features(train_instance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "#Computing prediction for feature vector (Problem 4.8 c)\n",
    "my_feature_vector = TestInstance(0, 0, 0, 0, 0, 1)\n",
    "pred = forest_prediction(forest2, my_feature_vector)\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
